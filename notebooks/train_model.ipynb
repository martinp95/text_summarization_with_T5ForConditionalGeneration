{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training T5 Model for Text Summarization\n",
    "\n",
    "This notebook demonstrates how to train a T5 model for text summarization using PyTorch Lightning. We will use the `SummarizationDataLoader` class to load and process the dataset and the `Summarizer` class to define the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and set up the environment\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to the system path\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the DataLoader and Summarizer classes\n",
    "from training import SummarizationDataLoader, Summarizer\n",
    "\n",
    "# Import PyTorch and PyTorch Lightning libraries\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Callbacks\n",
    "\n",
    "We will define two callbacks:\n",
    "- `ModelCheckpoint`: Saves the best model based on the validation loss.\n",
    "- `EarlyStopping`: Stops training if the validation loss does not improve for a specified number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ModelCheckpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='../checkpoints/',  # Path to save the models\n",
    "    filename='best-checkpoint',\n",
    "    save_top_k=1,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# Define the EarlyStopping callback\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    mode='min'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize DataLoader and Model\n",
    "\n",
    "We will initialize the `SummarizationDataLoader` with a batch size of 8 and the `Summarizer` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# Initialize the DataLoader with batch size 8\n",
    "data_module = SummarizationDataLoader(batch_size=8)\n",
    "\n",
    "# Initialize the Summarizer model\n",
    "model = Summarizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "We will set up the `Trainer` with the defined callbacks and train the model for a maximum of 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\Martin\\miniconda3\\envs\\text_summarization_with_T5ForConditionalGeneration\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Trainer with the callbacks\n",
    "trainer = Trainer(\n",
    "    max_epochs=10,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "    devices=1 if torch.cuda.is_available() else 1,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to training mode\n",
    "model.train()\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Best Model\n",
    "\n",
    "After training, we will load the best model saved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model has been load from : D:\\Estudios\\Proyectos\\text_summarization_with_T5ForConditionalGeneration\\checkpoints\\best-checkpoint.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Load the best model checkpoint\n",
    "best_model_path = os.path.join(\n",
    "    checkpoint_callback.dirpath,\n",
    "    checkpoint_callback.filename + '.ckpt'\n",
    ")\n",
    "\n",
    "# Check if the best model path is not empty\n",
    "if best_model_path:\n",
    "    # Load the model from the checkpoint\n",
    "    model = Summarizer.load_from_checkpoint(best_model_path)\n",
    "    print(f\"The best model has been load from : {best_model_path}\")\n",
    "else:\n",
    "    print(\"No best model checkpoint found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "\n",
    "We will ensure the model is in evaluation mode and evaluate it with test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "trainer.test(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Model and Tokenizer\n",
    "\n",
    "We save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../fine_tuned/fine_tuned_t5\\\\tokenizer_config.json',\n",
       " '../fine_tuned/fine_tuned_t5\\\\special_tokens_map.json',\n",
       " '../fine_tuned/fine_tuned_t5\\\\spiece.model',\n",
       " '../fine_tuned/fine_tuned_t5\\\\added_tokens.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.save_pretrained(\"../fine_tuned/fine_tuned_t5\")\n",
    "model.tokenizer.save_pretrained(\"../fine_tuned/fine_tuned_t5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"../fine_tuned/fine_tuned_t5\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"../fine_tuned/fine_tuned_t5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_text(text:str) -> str:\n",
    "    inputs = tokenizer.encode(\n",
    "                \"summarize: \" + text,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=1024,\n",
    "                truncation=True\n",
    "            )\n",
    "    summary_ids = model.generate(\n",
    "                inputs,\n",
    "                max_length=128,\n",
    "                min_length=40,\n",
    "                length_penalty=2.0,\n",
    "                num_beams=4,\n",
    "                early_stopping=True\n",
    "            )\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: Business entity is a firm, corporation, association, partnership, partnership, consortium, joint venture, or other form of enterprise. Defines \"business entity\" as a firm, corporation, association, association, partnership, partnership, consortium, joint venture, or other form of enterprise.\n"
     ]
    }
   ],
   "source": [
    "# Example input text for summarization\n",
    "input_text = \"\"\"SECTION 1. LIABILITY OF BUSINESS ENTITIES PROVIDING USE OF FACILITIES TO NONPROFIT ORGANIZATIONS.\n",
    "(a) Definitions.--In this section:\n",
    "    (1) Business entity.--The term \"business entity\" means a firm, corporation, association, partnership, consortium, joint venture, or other form of enterprise.\n",
    "    (2) Facility.--The term \"facility\" means any real property, including any building, improvement, or appurtenance.\n",
    "    (3) Gross negligence.--The term \"gross negligence\" means voluntary and conscious conduct by a person with knowledge (at the time of the conduct) that the conduct is likely to be harmful to the health or well-being of another person.\n",
    "    (4) Intentional misconduct.--The term \"intentional misconduct\" means conduct by a person with knowledge (at the time of the conduct) that the conduct is harmful to the health or well-being of another person.\n",
    "    (5) Nonprofit organization.--The term \"nonprofit organization\" means:\n",
    "        (A) any organization described in section 501(c)(3) of the Internal Revenue Code of 1986 and exempt from tax under section 501(a) of such Code; or\n",
    "        (B) any not-for-profit organization organized and conducted for public benefit and operated primarily for charitable, civic, educational, religious, welfare, or health purposes.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Summary:\", summary_text(input_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text_summarization_with_T5ForConditionalGeneration",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
